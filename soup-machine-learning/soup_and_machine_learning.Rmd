---
title: "Everything I Know About Machine Learning I Learned from Making Soup"
author: "Brad Stieber"
date: "July 13, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE,
                      error = FALSE)
```


# Introduction

In this post, I'm going to make the claim that parts of the machine learning process can be simplified by using the analogy of making soup. Relying on some insight from the [CRISP-DM framework](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining), my own experience as an amateur chef, and the well-known [iris data set](https://archive.ics.uci.edu/ml/datasets/iris), I'm going to try to convince you that the soup and machine learning connection is a pretty decent first approximation you could use while explaining the machine learning process.

This post is pretty light on code, with just a little bit of it at the end for illustrative purposes. These are the packages we'll need.

```{r}
library(tidyverse)
library(glmnet)
library(caret) # caret or carrot?
```


# Some Background

<p><a href="https://commons.wikimedia.org/wiki/File:CRISP-DM_Process_Diagram.png#/media/File:CRISP-DM_Process_Diagram.png"><img src="https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png" alt="CRISP-DM Process Diagram.png" height="400" width="400"></a><br> The CRISP-DM Framework (Kenneth Jensen) <a href="https://creativecommons.org/licenses/by-sa/3.0" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=24930610">Link</a></p>

I recently gave a presentation on the CRISP-DM framework to the various teams that make up the IT Department at my organization. While I was discussing the Modeling phase of CRISP-DM, I got some questions that come up fairly often when you talk about data science with a fairly technical audience.

_When you're building a model, what are you doing? Where are you spending your time? How long does that take?_

The people in IT know that data, machine learning, and artificial intelligence are impacting our daily lives, from [chat bots](https://www.kdnuggets.com/2017/08/deep-learning-train-chatbot-talk-like-me.html) to [spam email detection](https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering) to the [curation of news feeds](http://fortune.com/facebook-machine-learning/). While they have tremendous awareness of the _impact_ of data science (along with technical acumen), they may not have as much awareness of the _processes_ of data science. I think the responsibility of demystifying machine learning rests on data scientists, and it's imperative to have comprehensible mental models that can be employed to describe and de-clutter the machine learning process.

I thought I did a fairly good job of breaking down the three components of machine learning and the typical amount of iteration within each component by mirroring the CRISP-DM breakdown:

  - Problem type and associated modeling technique
      - Iteration level: low
  - Parameter tuning
      - Iteration level: high
  - Feature engineering and selection
      - Iteration level: high
  
Of course, I was in a room full of very smart people, trying to extemporaneously explain parameter tuning and feature engineering in a coherent way, so I probably could have done a better job.

A few minutes after the meeting, I realized I could have used a very simple analogy to explain the machine learning process.

__Machine learning is like making a soup. First, you pick the type of soup you want to make. Second, you figure out what types of ingredients are going to be in the soup and how these ingredients are going to be prepped. Third, you determine how you're going to cook the soup. Finally, you taste the soup and iterate to hopefully make it taste better.__

# Making a Soup = Machine Learning

While I'm certainly not an expert chef, I think you can boil down making a soup into a few simple components.

<iframe src="https://giphy.com/embed/xT9DPhWvvzbSI5vrYQ" width="480" height="270" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/cravetvcanada-seinfeld-xT9DPhWvvzbSI5vrYQ">via GIPHY</a></p>

## Picking the Soup = Selecting a Modeling Technique

In this step, we're just trying to figure out what we want to make (and hopefully enjoy). In the machine learning world, this is where we need to think carefully about the problem we're trying to solve, and which of the many machine learning algorithms can be used to attack the problem.

__Soup Making:__ what type of soup are we trying to make? are there external characteristics (season, weather, mood) we should consider? what soup will we enjoy? how difficult is this soup to make?

__Machine Learning:__ what type of question are we trying to answer? what type of model will allow us to use data to answer this question? how is this model implemented? what are its assumptions?

## Ingredients = Feature Engineering & Selection

Now that we've decided what we're going to make, we need to head to the grocery store and pick up the ingredients. After that we'll need to prepare the ingredients for cooking. Similarly, we'll need to understand our data set, and create/transform/aggregate our variables to get them into a useful form for modeling.

__Soup Making:__ what vegetables are needed and how should they be prepped? what type of protein will we be using? is the soup butter-based, cream-based or something else?

__Machine Learning:__ what variables are needed for this model? do we need to standardize any of the variables? are there non-numeric variables? if so, how should those be handled?

## Cooking Methods = Parameter Tuning

Once we've decided upon the type of soup and the ingredient, it comes time to actually make the soup. This step involves select the best combination of different cooking methods to make the optimal (i.e. most tasty) soup. When we perform parameter tuning, we're trying to pick the best combination of values to make our machine learning algorithms reach optimal performance. These values are different from the variables we previously discussed, as the variables are the _inputs_ for our ML algorithms, while the tuning parameters describe the algorithm itself.

__Soup Making:__ what heat are we cooking at and for how long? is the pot covered or uncovered? will the pot be on the stove top for the entirety of cooking or will we move it to the oven? how long will we let the soup simmer? how big of a batch are we making?

__Machine Learning:__ what is our loss function? is there a learning rate? what's the k in our k-fold cross validation? how many trees in our random forest? how much should we penalize complexity? what is the training/validation split? does an ensemble model outperform a single model?

# Building a Model

Let's see this framework in action. I'm going to pick a fairly simple classification task to demonstrate.

I'm going to use the iris dataset and try to predict whether a flower is from the setosa species.

Here's a quick look at the data

```{r echo = FALSE}
knitr::kable(head(iris[sample(nrow(iris), 6), ]), row.names = FALSE)
```

```{r echo = FALSE}
iris_colors <- RColorBrewer::brewer.pal(3, 'Set1')[iris$Species]


pairs(iris[, 1:4], col = iris_colors, oma=c(3,3,3,15))
par(xpd = TRUE)
legend("bottomright", 
       fill = unique(iris_colors), 
       legend = c(levels(iris$Species)))
```

The iris data is used as a "hello, world" in data science. It has nice applications across a broad spectrum of applications: [clustering](https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html), [regression](https://warwick.ac.uk/fac/sci/moac/people/students/peter_cock/r/iris_lm/), [classification](http://michael.hahsler.net/SMU/EMIS7332/R/logistic_regression.html), and [visualization](https://bl.ocks.org/mbostock/4063663). It's worth getting [excited](https://eagereyes.org/blog/2018/how-to-get-excited-about-standard-datasets) about!

## Picking the Soup

By design, the problem we're trying to tackle is predicting if a flower is from the setosa species. Since we already know that there is something we're specifically trying to predict, we only have to explore supervised machine learning algorithms. We also know that we're not really interested in building a model which generates predictions on a continuous range. We only want the answer a _binary_ question: is this a setosa or not?

This narrows the set of algorithms even further, and we only need to explore models which will either generate a classification for a flower, or will generate a probability of the flower's species being setosa.

For this post, I'm going to use [elastic net](https://en.wikipedia.org/wiki/Elastic_net_regularization) logistic regression. Elastic net fits a logistic regression while also penalizing the complexity of the model. The excellent [__`glmnet`__](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html) package allows us to build models in this fashion. I'm going to use ridge regression (elastic net with $\alpha=0$), which penalizes the $L_2$ norm of the coefficients. Fitting an elastic net with $\alpha=1$ generates a LASSO model, which can also be useful for variable selection, but I'm using a different technique to do variable selection in this post, so I'm sticking with ridge here. 

## Ingredients

Feature engineering and selection is one of the most time consuming parts of the machine learning process. To keep this post brief, I'm going to go through a few feature engineering and selection steps.

First, we split the data into [training and testing](https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets) sets. After this, I extract only the numeric predictor variables for the model, and then add squared terms as well as interactions between each of the first order effects. These two steps transforms the predictor matrix from four columns into fourteen, more than tripling the original size. Finally, we use the `preProcess` function from the [__`caret`__](https://topepo.github.io/caret/index.html) package to center and scale each numeric variable so that it has zero mean and unit variance.

```{r}
set.seed(123)
# create training and testing split
training_index <- sample(nrow(iris), 0.7 * nrow(iris))
iris_train <- iris[training_index,]
iris_test <- iris[-training_index,]
# grab X data add squared term for each column
iris_X_train <- iris_train[,-5] %>%
  mutate_all(funs('sq' = . ^ 2))
iris_X_test <- iris_test[,-5] %>%
  mutate_all(funs('sq' = . ^ 2))
# all two way interactions with first order terms
iris_X_train <- 
  model.matrix(~-1 + . + (Sepal.Length+Sepal.Width+Petal.Length+Petal.Width) ^ 2,
               data = iris_X_train)
iris_X_test <-
  model.matrix(~-1 + . + (Sepal.Length+Sepal.Width+Petal.Length+Petal.Width) ^ 2,
               data = iris_X_test)
# fit center and scale processor on training data
preProc <- preProcess(iris_X_train)
iris_X_train_cs <- predict(preProc, iris_X_train)
iris_X_test_cs <- predict(preProc, iris_X_test)
# labels, convert to factor for glmnet
iris_y_train <- as.factor(as.numeric(iris_train$Species == 'setosa'))
iris_y_test <- as.factor(iris_test$Species == 'setosa')
```

The data augmentation steps I went through only created numeric variables, but often times we'll use decision trees or similar techniques to create categorical variables from numeric. We use those methods when it's unnecessary to retain the continuous nature of a numeric variable (often the case with age or physiological measurements like height or weight).

After we've gone through the feature engineering step, we can think about which variables we'll actually want to use in our model. There can be considerable back-and-forth between feature engineering and feature selection, just like iterating on a recipe may involve different ingredients and different ways of preparing those ingredients.

To do feature selection, I'm going to once again turn to the __`caret`__ package and use the `rfe` function. We could also use the infrastructure of the __`glmnet`__ package to do some feature selection, as the [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics) helps us perform variable selection. 

```{r}
set.seed(123)
rf_control <- rfeControl(rfFuncs, method = 'cv', number = 5)

iris_rfe <- rfe(x = iris_X_train_cs, 
                y = iris_y_train,
                sizes = 2:14, # select at least two variables
                rfeControl = rf_control)

iris_rfe$optVariables
```

The procedure we used for variable selection is pretty straightforward, and suggested an interaction between the petal length and petal width variables, and petal length squared. It's generally a bad idea to include higher ordered terms without also including the lower order terms, so our final model will have three variables: petal width, petal length, and petal width * petal length.


## Cooking Methods

In elastic net regression, we have two parameters to select: $\alpha$ and $\lambda$. $\alpha$ controls the weight we give to the $L_1$ and $L_2$ penalties (with $\alpha$ being placed on the $L_1$ norm, and $1-\alpha$ being placed on the $L_2$ norm). Putting all of the weight on $L_2$ norm is better known as [Tikhonov regularization or ridge regression](https://en.wikipedia.org/wiki/Tikhonov_regularization). I've already made my choice of $\alpha$ for this post, so we don't have to tune it.

The other parameter we need to tune is $\lambda$, which controls the overall strength of the penalty we'll place on the $L_2$ norm of the coefficients. A higher $\lambda$ value will "shrink" the model's coefficients, whereas a smaller $\lambda$ value will result in a model more similar to the standard un-regularized logistic regression fit.

As we'll see, the `cv.glmnet` function allows us to use cross-validation to tune $\lambda$. 

```{r}
vv <- c("Petal.Width", "Petal.Length", "Petal.Length:Petal.Width")

iris_X_train_cs_sub <- iris_X_train_cs[, colnames(iris_X_train_cs) %in% vv]
iris_X_test_cs_sub <- iris_X_test_cs[, colnames(iris_X_test_cs) %in% vv]
```


```{r}
cv_glm1 <- cv.glmnet(x = iris_X_train_cs_sub,
                     y = iris_y_train,
                     alpha = 0,
                     family = 'binomial',
                     nfolds = length(iris_y_train) - 1,# LOOCV
                     standardize = FALSE,
                     intercept = FALSE) 

plot(cv_glm1)
lambda_1se_1 <- cv_glm1$lambda.1se

cv_glm2 <- cv.glmnet(x = iris_X_train_cs_sub,
                     y = iris_y_train,
                     alpha = 0,
                     lambda = exp(seq(-10, log(lambda_1se_1), length.out = 500)),
                     family = 'binomial',
                     nfolds = length(iris_y_train) - 1,# LOOCV
                     standardize = FALSE,
                     intercept = FALSE)

plot(cv_glm2)
abline(v = lambda_1se_1)

lambda_2 <- exp(-6.5)
```


## Tasting the soup

```{r}
# naive classification accuracy
class_acc <- function(preds, labels, thresh = 0.5){
  tt <- table(preds > thresh, labels)
  
  sum(diag(tt)) / sum(tt)
}
```


```{r}
ridge_1 <- glmnet(x = iris_X_train_cs_sub, y = iris_y_train,
                  family = 'binomial', lambda = lambda_1se_1, alpha = 0,
                  standardize = FALSE, intercept = FALSE)

ridge_2 <- glmnet(x = iris_X_train_cs_sub, y = iris_y_train,
                  family = 'binomial', lambda = lambda_2, alpha = 0,
                  standardize = FALSE, intercept = FALSE)

glm1 <- glm(iris_y_train ~ -1 + ., data = data.frame(iris_X_train_cs_sub), 
            family = 'binomial')

glm2 <- glm(iris_y_train ~ -1 + Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
            data = data.frame(iris_X_train_cs), 
            family = 'binomial') 
```





# Wrapping Up

When we compare making a soup to machine learning, we get a fairly simple and understandable lens through which we can look at machine learning. Just like making soup or cooking in general, iteration is a key component of machine learning. If you ask anyone that's trying to develop a recipe, they probably won't get it right the first time. If they do get it right the first time, maybe that's because

  - they got lucky
  - they aren't trying to make too difficult of a dish
  - they're an experienced cook
  
These situations have clear parallels to machine learning (maybe you got lucky or maybe all you need is a simple model or maybe you're an experience data scientist).

I feel like this analogy is a pretty straightforward way to explain machine learning to a broad audience of people that are interested in the topic. I found a [similar article](https://www.becomingadatascientist.com/2017/07/17/introductory-machine-learning-terminology-with-food/) which discussed ideas that are related to the ones I talked about in my post. If you know of any other posts with similar sentiments, I hope you'll share it with me!

Thanks for reading my post and leave a comment below if you have any thoughts or feedback!









