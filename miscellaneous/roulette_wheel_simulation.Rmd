---
title: 'Roulette Wheel for Multi-Armed Bandits: A Simulation in R'
author: "Brad Stieber"
date: "10/29/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r}
library(tidyverse)
theme_set(theme_bw())
```


# Introduction 

One of my favorite [data science blogs](https://jamesmccaffrey.wordpress.com/) comes from James McCaffrey, a software engineer and researcher at Microsoft. He recently wrote a [blog post](https://jamesmccaffrey.wordpress.com/2019/10/28/roulette-wheel-selection-for-multi-armed-bandit-problems/) on a method for allocating turns in a multi-armed bandit problem.

I really liked his post, and decided to take a look at the algorithm he described and code up a function to do the simulation in R.

__Note:__ this is strictly an implementation of Dr. McCaffrey's ideas from his blog post, and should not be taken as my own.

# Background

The basic idea of a [multi-armed bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit) is that you have a fixed number of resources (e.g. money at a casino) and you have a number of competing places where you can allocate those resources (e.g. four slot machines at the casino). These allocations occur sequentially, so in the casino example, we choose a slot machine, observe the success or failure from our play, and then make the next allocation decision. Since we're data scientists at a casino, hopefully we're using the information we're gathering to make better gambling decisions (is that an oxymoron?).

We want to choose the best place to allocate our resources, and maximize our reward for each allocation. However, we should shy away from a greedy strategy (just play the winner), because it doesn't allow us to explore our other options.

There are [different strategies](https://en.wikipedia.org/wiki/Multi-armed_bandit) for choosing where to allocate your next resource. One of the more popular choices is Thompson sampling, which usually involves sampling from a Beta distribution, and using the results of that sampling to determine your next allocation (out of scope for this blog post!).

# Code: `roulette_wheel`

The following function implements the roulette wheel allocation, for a flexible number of slot machines.

The function starts by generating a warm start with the data. We need to gather information about our different slot machines, so we allocate a small number of resources to each one to collect information. After we do this, we start the real allocation. We pick a winner based on how its cumulative probability compares to a draw from a random uniform distribution.

So, if our observed success probabilities are

```{r}
knitr::kable(
  data.frame(
    machine = 1:3,
    observed_prob = c(.2, .3, .5),
    cumulative_prob = cumsum(c(.2, .3, .5))
  )
)
```

And our draw from the random uniform was 0.7, we'd pick the third arm.

We then continue this process (playing a slot machine, observing the outcome, recalculating observed probabilities, and picking the next slot machine) until we run out of coins.

```{r echo = TRUE}
roulette_wheel <- function(coins = 40, 
                           starts = 5,
                           true_prob = c(0.3, 0.5, 0.7)){
  # must have enough coins to generate initial empirical distribution
  if (coins < (length(true_prob) * starts)){
    stop("To generate a starting distribution, each machine must be",
         " played ",
         starts,
         " times - not enough coins to do so.")
  }
  # allocate first ("warm up")
  SS <- sapply(true_prob, FUN = function(x) sum(rbinom(starts, 1, x)))
  FF <- starts - SS
  # calculate metrics used for play allocation
  probs <- SS / (SS + FF)
  probs_normalized <- probs / sum(probs)
  cumu_probs_normalized <- cumsum(probs_normalized)
  # update number of coins
  coins <- coins - (length(true_prob) * starts)
  # create simulation data.frame
  sim_df <- data.frame(machine = seq_along(true_prob),
                       true_probabilities = true_prob,
                       observed_probs = probs,
                       successes = SS,
                       failures = FF,
                       plays = SS + FF,
                       machine_played = NA,
                       coins_left = coins)
  # initialize before while loop
  sim_list <- vector('list', length = coins)
  i <- 1
  # play until we run out of original coins
  while(coins > 0){
    # which machine to play?
    update_index <- findInterval(runif(1), c(0, cumu_probs_normalized))
    # play machine
    flip <- rbinom(1, 1, true_prob[update_index])
    # update successes and failure for machine that was played
    SS[update_index] <- SS[update_index] + flip
    FF[update_index] <- FF[update_index] + (1-flip)
    # update metrics used for play allocation
    probs <- SS / (SS + FF)
    probs_normalized <- probs / sum(probs)
    cumu_probs_normalized <- cumsum(probs_normalized)
    # update number of coins
    coins <- coins - 1    
    # update simulation data.frame (very inefficient)
    sim_list[[i]] <- data.frame(machine = seq_along(true_prob),
                                true_probabilities = true_prob,
                                observed_probs = probs,
                                successes = SS,
                                failures = FF,
                                plays = SS + FF,
                                machine_played = seq_along(true_prob) == update_index,
                                coins_left = coins)
    i <- i + 1
  }
  # show success:failure ratio
  message("Success to failure ratio was ",
          round(sum(SS) / sum(FF), 2),
          "\n",
          paste0("(", 
                 paste0(SS, collapse = "+"), 
                 ")/(", 
                 paste0(FF, collapse = "+"), ")"))
  # return data frame of values from experiment
  rbind(sim_df, do.call('rbind', sim_list))
}
```

# Data Analysis

I'll show a brief example of what we can do with the data generated from this function.

```{r echo = TRUE, cache = TRUE}
set.seed(123)
rw1 <- roulette_wheel(coins = 5000, 
                      starts = 10, 
                      true_prob = c(0.1, 0.25, 0.5, 0.65))
```

```{r}
knitr::kable(rw1[rw1$coins_left == 0,], 
             caption = 'Final simulation result') # show last play
```

Let's look at how the observed probabilities changed over time:

```{r fig.width = 8, fig.height=6}
library(scales)

p1 <- rw1 %>%
  ggplot(aes(40 + max(coins_left) - coins_left, observed_probs))+
  geom_hline(aes(yintercept = true_probabilities, colour = factor(machine)),
             linetype = 'dashed')+
  geom_line(aes(colour = factor(machine)), size = 1.2)+
  scale_colour_viridis_d(name = "Machine")

p1 <- p1 +
  xlab("Simulation")+
  scale_y_continuous("Chance of Success", labels = percent)+
  ggtitle("How did observed probabilities of success change during the simulation?",
          subtitle = 'Dashed line represent true probabilities of success')

p1
```

And how did our plays for each machine accumulate through time?

```{r fig.width=8, fig.height=6}
p2 <- rw1 %>%
  ggplot(aes(40 + max(coins_left) - coins_left, plays))+
  geom_line(aes(colour = factor(machine)), size = 1.2)+
  scale_colour_viridis_d(name = "Machine")

p2 <- p2 +
  xlab("Simulation")+
  scale_y_continuous("Cumulative # of Plays", labels = comma)+
  ggtitle("How did cumulative number of plays change during the simulation?")

p2
```

Boring! If we run a smaller number of variations, we might get a better sense of variation in our number of plays.

```{r echo = TRUE}
set.seed(123)
rw2 <- roulette_wheel(coins = 100, 
                      starts = 5, 
                      true_prob = c(0.1, 0.25, 0.5, 0.65))
```

```{r}
knitr::kable(rw2[rw2$coins_left == 0,], 
             caption = 'Final simulation result') # show last play
```

```{r}
p3 <- rw2 %>%
  ggplot(aes(20 + max(coins_left) - coins_left, plays))+
  geom_line(aes(colour = factor(machine)), size = 1.2)+
  scale_colour_viridis_d(name = "Machine")

p3 <- p3 +
  xlab("Simulation")+
  scale_y_continuous("Cumulative # of Plays", labels = comma)+
  ggtitle("How did cumulative number of plays change during the simulation?")

p3
```

We can see that machine 1 was never played after the warm start, which represents one potential drawback of this approach.

# Conclusion

This was a fun exercise for me, and it reminded me of a [presentation](https://github.com/bgstieber/files_for_blog/blob/master/miscellaneous/MultiArmedBandits.pdf) I did in graduate school about a very similar topic. I also wrote a [roulette wheel function](https://github.com/bgstieber/files_for_blog/blob/master/miscellaneous/roulette_wheel.py) in Python, and was moderately successful at that (it runs faster than my R function, but I'm less confident in how "pythonic" it is).

My biggest concern with this implementation is the potential situation in which our warm start results in all failures for a given slot machine. If the machine fails across the warm start, it will not be selected for the rest of the simulation. To offset this, you could add a little "jitter" (technical term: epsilon) to the observed probabilities at each iteration. Another option would be to generate a second random uniform variable, and if that value is very small, you that pull a random lever, rather than the one determined by the simulation.


