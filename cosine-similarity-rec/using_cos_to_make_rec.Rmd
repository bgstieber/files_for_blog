---
title: "Recommending Songs Using Cosine Similarity in R"
author: "Brad Stieber"
date: "December 29, 2018"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

Recommendation engines impact a large amount of our online lives. The content we watch on Netflix, the products we purchase on Amazon, and even [the homes we buy](https://www.redfin.com/blog/2013/09/the-end-of-search.html) are all served up using recommendation engines. In this post, I'll run through one of the key metrics used in recommendation engines: cosine similarity.

## Introduction

There are a few different flavors of recommendation engines. One type is __collaborative filtering__, which relies on the behavior of users to understand and predict the similarity between items. There are two subtypes of collaborative filtering: __user-user__ and __item-item__. In a nutshell, user-user recommendation engines will look for similar users to you, and recommend things that these similar users have liked. Item-item recommendation engines generate predictions based on the similarity of items instead of the similarity of users. Converting an engine from user-user to item-item engines can reduce the amount of computational cost in generating recommendations.

Another type of recommendation engine is __content-based__. Rather than using the behavior of other users, or the similarity between ratings, content-based systems employ information about the items themselves (e.g. genre, starring actors, when the movie was released). Then, a user's behavior is examined to generate a user profile, which tries to find content similar to what's been consumed before based on the characteristics of the new content.

Cosine similarity is helpful for building both types of recommender systems. In this post, we'll be using it to generate song recommendations based on how often users listen to different songs.

The only package we'll need for this post is:

```{r}
library(tidyverse)
```


## The Math: Cosine Similarity

Growing up, I never really liked geometry. During grad school, I learned how important it was to have a decent grasp of some basic geometric concepts, and I began to appreciate the power of visualizing information to improve geometric understanding.

Cosine similarity is built on the geometric definition of the [__dot product__](https://en.wikipedia.org/wiki/Dot_product) of two vectors.

$$a \cdot b = a^{T}b = \sum_{i=1}^{n} a_i b_i $$

You may be wondering what $a$ and $b$ actually represent. If we're trying to recommend certain products, $a$ and $b$ might be the collection of ratings from three users for two products. For example, if $a =[5, 0, 1]$ and $b = [0, 1, 2]$, the first customer rated $a$ a 5 and did not rate $b$, the second customer did not rate $a$ and gave $b$ a 1, and the third customer rated $a$ a 1 and $b$ a 2.

With that out of the way, we can layer in geometric information

$$a \cdot b = ||a|| \, ||b|| \text{cos}(\theta)$$
where $\theta$ is the angle between $a$ and $b$ and $||x||$ is the magnitude/length/norm of a vector $x$. From the above expression, we can arrive at cosine similarity:

$$ \text{cos}(\theta) = \frac{a \cdot b}{||a|| \, ||b||}$$

In `R` this is defined as:

```{r}
cosine_sim <- function(x, y) crossprod(x,y)/sqrt(crossprod(x)*crossprod(y))
```

OK, OK, OK, there's the formula, and there's the `R` function, but where's the intuition? What does it all mean?

What I like to focus on in cosine similarity is the angle $\theta$. This basically tells us how far we'd have to move vector $a$ so that it could rest on top of $b$, assuming we can only adjust the orientation of $a$, and have no ability to influence its magnitude. The easier it is to get $a$ on top of $b$, the smaller this angle will be. The smaller $\theta$ is, the larger $\text{cos}(\theta)$ will be. [This blog post](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/) has a great image demonstrating cosine similarity for a few examples.

<br>

![_Image from a 2013 [blog post](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/) by Christian S. Perone_](http://blog.christianperone.com/wp-content/uploads/2013/09/cosinesimilarityfq1.png)

<br>

For the data we'll be looking at in this post, $\text{cos}(\theta)$ will be somewhere between 0 and 1, since the data we're looking at is all non-negative. In some cases, there may be data which is positive _and_ negative, in which case $\text{cos}(\theta)$ will be between -1 and 1, with -1 meaning $a$ and $b$ are perfectly dissimilar and 1 meaning $a$ and $b$ are perfectly similar.


## The Data: Recommending Songs

We use data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). We're working with a subset of the data (only 10K songs), but that should be enough for us.

```{r echo = TRUE, cache = TRUE}
# read user play data and song data from the internet
play_data <- "https://static.turi.com/datasets/millionsong/10000.txt" %>%
  read_tsv(col_names = c('user', 'song_id', 'plays'))

song_data <- 'https://static.turi.com/datasets/millionsong/song_data.csv' %>%
  read_csv() %>%
  distinct(song_id, title, artist_name)
# join user and song data together
all_data <- play_data %>%
  group_by(user, song_id) %>%
  summarise(plays = sum(plays, na.rm = TRUE)) %>%
  inner_join(song_data)
```

Here are the first few rows of the data:

```{r}
head(all_data)
```

Now, there happen to be a lot of users in this data set, so combining the number of users with songs makes the data a little too unwieldy for this toy example. We're going to filter our dataset so that it's only based on the top 1,000 songs. We use the `spread` function to turn our data from being "tall" (one row per user per song) to being "wide" (one row per user, and one column per song).

```{r cache=TRUE}
top_1k_songs <- all_data %>%
    group_by(song_id, title, artist_name) %>%
    summarise(sum_plays = sum(plays)) %>%
    ungroup() %>%
    top_n(1000, sum_plays) %>% 
    distinct(song_id)

all_data_top_1k <- all_data %>%
  inner_join(top_1k_songs)

top_1k_wide <- all_data_top_1k %>%
    ungroup() %>%
    distinct(user, song_id, plays) %>%
    spread(song_id, plays, fill = 0)

ratings <- as.matrix(top_1k_wide[,-1])

```

Here's a sample of what the `ratings` matrix looks like:

```{r}
ratings[1:5, 1:5] # one row per user, one column per song
```


I then wrote a function called `calc_cos_sim`, which will calculate the similarity between a chosen song and the other songs, and recommend 5 new songs for a user to listen to.

```{r}
calc_cos_sim <- function(song_code, 
                         rating_mat = ratings,
                         songs = song_data,
                         return_n = 5) {
  # find our song
  song_col_index <- which(colnames(rating_mat) == song_code)
  # calculate cosine similarity for each song based on 
  # number of plays for users
  cos_sims <- apply(rating_mat, 2,
                    FUN = function(y) cosine_sim(rating_mat[,song_col_index],
                                                 y))
  # return results
  data_frame(song_id = names(cos_sims),
             cos_sim = cos_sims) %>%
    filter(song_id != song_code) %>% # remove self reference
    inner_join(songs) %>%
    arrange(desc(cos_sim)) %>%
    top_n(return_n, cos_sim)
  
}
```

We can use the function above to calculate similarities for a few songs.

Let's look at the hip-hop classic "Forgot about Dre" first.

```{r}
forgot_about_dre <- 'SOPJLFV12A6701C797'
knitr::kable(calc_cos_sim(forgot_about_dre))
```

Each song we recommended is a hip-hop song, which is a good start! Even on this reduced dataset, the engine is making _decent_ recommendations. 

<!--
```{r}
enter_sandman <- 'SOCHYVZ12A6D4F5908'
knitr::kable(calc_cos_sim(enter_sandman))
```
-->

The next song is "Come As You Are" by Nirvana. Users who enjoy this song probably like other grunge/rock songs.

```{r}
come_as_you_are <- 'SODEOCO12A6701E922'
knitr::kable(calc_cos_sim(come_as_you_are))
```

Alright, 2 for 2. One thing to be mindful of when looking at these results is that we're not incorporating _any_ information about the songs themselves. Our engine isn't built using any data about the artist, genre, or other musical characteristics. Additionally, we're not considering any demographic information about the users, and it's fairly easy to see how useful age, gender, and other user-level data could be in making recommendations. If we used this information in addition to our user play data, we'd have what is called a __hybrid recommendation system__.

Finally, we'll recommend songs for our hard-partying friends that like the song "Shots" by LMFAO featuring Lil Jon.

```{r}
shots <- 'SOJYBJZ12AB01801D0'
knitr::kable(calc_cos_sim(shots))
```

Well, the "16 Candles" result is a little surprising, but this might give us some insight into the demographics of users that like "Shots". The other four recommendations seem pretty solid. 

## Conclusion

Cosine similarity is simple to calculate and is fairly intuitive once some basic geometric concepts are understood. The simplicity of this metric makes it a great first-pass option for recommendation systems, and can be treated as a baseline with which to compare more computationally intensive and/or difficult to understand methods.

Recommendation systems will continue to play a large role in our online lives. I think it can be helpful to understand the components underneath these systems, so that we can treat them less as blackbox oracles and more as the imperfect prediction systems based on data they are.

I hope you liked this brief excursion into the world of recommendation engines. Hopefully you can walk away knowing a little more about why Amazon told you to buy that Hello Kitty backpack!

### Other Resources

Here are a few great resources if you want to dive deeper into cosine similarity. 

- [Wikipedia: Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)
- [Machine Learning :: Cosine Similarity for Vector Space Models (Part III)](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/)
- [Introduction to Collaborative Filtering](https://hackernoon.com/introduction-to-recommender-system-part-1-collaborative-filtering-singular-value-decomposition-44c9659c5e75)
- [What are Product Recommendation Engines? And the various versions of them?](https://towardsdatascience.com/what-are-product-recommendation-engines-and-the-various-versions-of-them-9dcab4ee26d5)
- [Implementing and Understanding Cosine Similarity](https://masongallo.github.io/machine/learning,/python/2016/07/29/cosine-similarity.html)
- [Series of blog posts about cosine similarity](http://stefansavev.com/blog/cosine-similarity-all-posts/)