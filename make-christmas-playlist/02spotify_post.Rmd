---
title: "Using Data Science to Make a Christmas Playlist that Doesn't Suck"
author: "Brad Stieber"
date: "November 22, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

In this post, I'm going to using data from Spotify to make a Christmas playlist that you'll actually like. We're going to employ a few standard data science techniques to __import__ data, __explore__ it, and finally __model__ the data.

Here are the steps we're going to take

1. Pull data into `R` using the [__`spotifyr`__](https://www.rcharlie.com/spotifyr/) package
1. Clean up the data using functions from the [__`caret`__](http://caret.r-forge.r-project.org/) package
1. Use principal components analysis to reduce the dimensionality of our data
1. Apply k-nearest neighbor and logistic regression modeling techniques to find Christmas songs that don't suck
1. Use our findings to create a playlist that you won't feel like turning off after ten minutes

Here are the packages we'll need:

```{r}
library(spotifyr)
library(tidyverse)
library(scales)
library(caret)
library(ggrepel)
library(RANN)
theme_set(theme_bw())
```


## Getting data from Spotify

The `spotifyr` package makes it really easy to grab data from Spotify. You can find a pretty good tutorial to get up-and-running from the [package's website](https://www.rcharlie.com/spotifyr/).

In the code below, we're going to connect to Spotify, and then pull all of my playlists. I created two playlists for this analysis. The first is called __holiday_playlist__, and it's a collection of 21 songs that I thought were pretty good. Then, we're going to pull in another playlist called __spotify_holiday_playlists__, which is a collection of 300+ holiday songs coming from a variety of Spotify holiday playlists (you can find the list of playlists I used [on my GitHub](https://github.com/bgstieber/files_for_blog/blob/master/make-christmas-playlist/spotify_playlists.txt)).

```{r}
# connect
access_token <- get_spotify_access_token()
```

```{r cache = TRUE, results=FALSE}
update <- 1 # change to update data
# get my playlists
my_playlists <- get_user_playlists('12102534356') 
# get playlist and playlist track info
## my curated list
holiday_playlist <- my_playlists  %>%
  filter(playlist_name == 'holiday_playlist') %>%
  get_playlist_tracks()

holiday_playlist_features <- holiday_playlist %>%
  get_track_audio_features()

## collection of spotify songs
spotify_holiday <- my_playlists %>%
  filter(playlist_name == 'spotify_holiday_playlists') %>%
  get_playlist_tracks() %>%
  # filter duplicates
  filter(!duplicated(.[c('track_name', 'artist_name')] %>%
           mutate_all(toupper)))

spotify_holiday_features <- spotify_holiday %>%
  get_track_audio_features()
```

Now we have all the data we'll need for this analysis. We have four `data_frame`s to play with. There are two containing qualitative information about the tracks (`holiday_playlist` and `spotify_holiday`) and two containing quantitative information (`holiday_playlist_features` and `spotify_holiday_features)`. 

The quantitative information has variables like __energy__ (a measure of the intensity and activity of the track) and __danceability__ (describes how suitable a track is for dancing). More information can be found in [the documentation for the Spotify API](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/).

## Data Processing

Let's take a look at the structure of `holiday_playlist_features`.

```{r}
str(holiday_playlist_features)
```

We can also visualize the quantitative variables. We'll look at both data sets, and the distributions should give us insight into whether we need to transform any of the variables (e.g. log-scale, square root transformations).

First, we'll look at the features from the playlist I hand-curated.

```{r echo = FALSE}
holiday_playlist_features %>%
  select_if(is.numeric) %>%
  gather(variable, value) %>%
  ggplot(aes(value, fill = variable))+
  geom_histogram()+
  facet_wrap(~variable, scales = 'free')+
  theme(legend.position = 'none')+
  ggtitle('Hand Curated Playlist Features',
          subtitle = "These songs don't suck")
```

Then we'll look at the playlist with candidate songs to be added to the Playlist that Doesn't Suck.

```{r echo = FALSE}
spotify_holiday_features %>%
  select_if(is.numeric) %>%
  gather(variable, value) %>%
  ggplot(aes(value, fill = variable))+
  geom_histogram()+
  facet_wrap(~variable, scales = 'free')+
  theme(legend.position = 'none')+
  ggtitle('Candidate Playlist Features',
          subtitle = "These songs might suck")
```

From the histograms above, we can make a few decisions about what we want to do with the quantitative variables:

1. Get rid of variables that seem quite discrete in nature (instrumentalness, speechiness, and time_signature)
1. Log-scale a few variables with right-skewed distributions (duration_ms, liveness, and tempo)
1. Center and scale variables to have mean zero and unit variance (this will make our PCA results more reasonable)

```{r}
vars_to_keep <- c("track_uri", "danceability", "energy", 
                  "loudness", "acousticness", "liveness", 
                  "valence", "tempo", "duration_ms")

vars_to_log1p <- c('duration_ms', 'liveness', 'tempo')

holiday_playlist_features2 <- holiday_playlist_features %>%
  select(vars_to_keep) %>%
  mutate_at(vars_to_log1p, log1p)

spotify_holiday_features2 <- spotify_holiday_features %>%
  select(vars_to_keep) %>%
  mutate_at(vars_to_log1p, log1p)

# center and scale, using my playlist as "training" set
pre_proc <- preProcess(holiday_playlist_features2,
                       method = c('center', 'scale'))

holiday_playlist_features2_scaled <- predict(pre_proc, 
                                             holiday_playlist_features2)

spotify_holiday_features2_scaled <- predict(pre_proc,
                                            spotify_holiday_features2)

```

Let's make the same historgrams as above, but with the centered and scaled values.

```{r echo = FALSE}
holiday_playlist_features2_scaled %>%
  select_if(is.numeric) %>%
  gather(variable, value) %>%
  ggplot(aes(value, fill = variable))+
  geom_histogram()+
  facet_wrap(~variable, scales = 'free')+
  theme(legend.position = 'none')+
  ggtitle('Hand Curated Playlist Features - Centered and Scaled',
          subtitle = "These songs don't suck")
```

```{r echo = FALSE}
spotify_holiday_features2_scaled %>%
  select_if(is.numeric) %>%
  gather(variable, value) %>%
  ggplot(aes(value, fill = variable))+
  geom_histogram()+
  facet_wrap(~variable, scales = 'free')+
  theme(legend.position = 'none')+
  ggtitle('Candidate Playlist Features - Centered and Scaled',
          subtitle = "These songs might suck")
```


## Dimensionality Reduction with PCA

We have 8 quantitiatve variables in the data set, which makes it challenging to visualize. We can use a [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) technique like principal components analysis to make our data easier to visualize.

Performing PCA in `R` is fairly straightforward. We're going to first run PCA on my curated playlist, and use the results to generate "predictions" for the larger playlist.

```{r}
# fit principal components (dimensionality reduction) on playlist
prco_holiday <- prcomp(holiday_playlist_features2_scaled[,-1])

pc_rotation <- prco_holiday$rotation %>%
  as_data_frame() %>%
  mutate(variable = row.names(prco_holiday$rotation))

training_pca <- predict(prco_holiday,
                        newdata = holiday_playlist_features2_scaled) %>%
  as_data_frame() %>%
  mutate(type = 'training') %>%
  bind_cols(holiday_playlist_features2_scaled)

testing_pca <- predict(prco_holiday, 
                       newdata = spotify_holiday_features2_scaled) %>%
  as_data_frame() %>%
  mutate(type = 'testing') %>%
  bind_cols(spotify_holiday_features2_scaled)

full_pca <- training_pca %>%
  bind_rows(testing_pca)
```

And now we can visualize the first two principal components, and look for clusters in the data.

```{r echo = FALSE}
full_pca %>%
  ggplot(aes(PC1, PC2, colour = type))+
  geom_point(size = 2, alpha = 0.7)+
  geom_point(data = filter(full_pca, type == 'training')) +
  scale_colour_manual(values = c('testing' = 'green',
                                 'training' = 'red'),
                      labels = c('candidates', 'curated'),
                      name = 'Song Type')+
  ggtitle('First Two Principal Components for Holiday Songs')
```

We can see that many of the songs from my holiday playlist have values for PC2 below zero (`r scales::percent(mean(training_pca$PC2 <= 0))`). Let's look at the variable loadings for the first two principal components. These loadings describe the weights that are multiplied by the variables to calculate the respective principal components.

```{r echo = FALSE}
prco_holiday$rotation %>%
    as.data.frame(stringsAsFactors = FALSE) %>%
    mutate(variable = row.names(.)) %>%
    select(variable, PC1, PC2) %>%
    mutate_if(is.numeric, round, 3) %>%
    arrange(desc(PC2)) %>%
  knitr::kable()
```

There are three variables which have positive loadings for PC2 (duration_ms, liveness, and energy). Since 

## Models

## Combining our Intelligence

## "Wrapping" Up

