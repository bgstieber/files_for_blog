by = c("year", "PLAYER NAME")) %>%
na.omit() %>%
filter(EVENTS >= 10) %>%
as_tibble() %>%
mutate(money_log = log(money)) %>%
mutate_at(c("SG_OFF_TEE", "SG_APPROACH", "SG_PUTTING","SG_AROUND_GREEN"),
funs(. * 10))
fitmod <- function(data) lm(money_log ~
SG_OFF_TEE+SG_APPROACH+SG_PUTTING+SG_AROUND_GREEN,
data = data)
sg_model <- sg_data2 %>%
group_by(year) %>%
nest() %>%
mutate(model = map(data, fitmod))
sg_model2 <- sg_model %>%
group_by(year)%>%
mutate(model_coef = map(model, broom::tidy),
model_glance = map(model, broom::glance))
sg_model2 %>%
unnest(model_coef) %>%
filter(term != "(Intercept)") %>%
ggplot(aes(year, estimate, colour = term))+geom_line()
exp(0.1)
sg_model2 %>%
unnest(model_coef) %>%
filter(term != "(Intercept)") %>%
ggplot(aes(year, estimate, colour = exp(term)))+geom_line()
sg_model2 %>%
unnest(model_coef) %>%
filter(term != "(Intercept)") %>%
ggplot(aes(year, exp(estimate), colour = term))+geom_line()
sg_model2 %>%
unnest(model_coef) %>%
filter(term != "(Intercept)") %>%
ggplot(aes(year, exp(estimate) - 1, colour = term))+geom_line()
sg_model2 %>%
unnest(model_coef) %>%
filter(term != "(Intercept)") %>%
ggplot(aes(year, exp(estimate) - 1, colour = term))+geom_line()+
scale_y_continuous(labels = percent)
sg_model2 %>%
unnest(model_coef) %>%
filter(term != "(Intercept)") %>%
ggplot(aes(year, exp(estimate) - 1, colour = term))+geom_line()+
scale_y_continuous(labels = percent,
"% Increase in Earnings from a 0.1 Strokes Gained Improvement")
?mutate_at
sg_data2 <- sg_data %>%
left_join(sg_off_tee %>%
select(year, `PLAYER NAME`, SG_OFF_TEE = AVERAGE),
by = c("year", "PLAYER NAME")) %>%
left_join(sg_approach %>%
select(year, `PLAYER NAME`, SG_APPROACH = AVERAGE),
by = c("year", "PLAYER NAME")) %>%
left_join(sg_putting %>%
select(year, `PLAYER NAME`, SG_PUTTING = AVERAGE),
by = c("year", "PLAYER NAME")) %>%
left_join(sg_around_green %>%
select(year, `PLAYER NAME`, SG_AROUND_GREEN = AVERAGE),
by = c("year", "PLAYER NAME")) %>%
left_join(avg_dist %>%
select(year, `PLAYER NAME`, AVG_DIST = AVG.),
by = c("year", "PLAYER NAME")) %>%
left_join(pct_total_money %>%
select(year, `PLAYER NAME`, money,
EVENTS),
by = c("year", "PLAYER NAME")) %>%
na.omit() %>%
filter(EVENTS >= 10) %>%
as_tibble() %>%
mutate(money_log = log(money)) %>%
mutate_at(c("SG_OFF_TEE", "SG_APPROACH", "SG_PUTTING","SG_AROUND_GREEN"),
funs("_tenth" = . * 10))
sg_data2
sg_data2 <- sg_data %>%
left_join(sg_off_tee %>%
select(year, `PLAYER NAME`, SG_OFF_TEE = AVERAGE),
by = c("year", "PLAYER NAME")) %>%
left_join(sg_approach %>%
select(year, `PLAYER NAME`, SG_APPROACH = AVERAGE),
by = c("year", "PLAYER NAME")) %>%
left_join(sg_putting %>%
select(year, `PLAYER NAME`, SG_PUTTING = AVERAGE),
by = c("year", "PLAYER NAME")) %>%
left_join(sg_around_green %>%
select(year, `PLAYER NAME`, SG_AROUND_GREEN = AVERAGE),
by = c("year", "PLAYER NAME")) %>%
left_join(avg_dist %>%
select(year, `PLAYER NAME`, AVG_DIST = AVG.),
by = c("year", "PLAYER NAME")) %>%
left_join(pct_total_money %>%
select(year, `PLAYER NAME`, money,
EVENTS),
by = c("year", "PLAYER NAME")) %>%
na.omit() %>%
filter(EVENTS >= 10) %>%
as_tibble() %>%
mutate(money_log = log(money)) %>%
mutate_at(c("SG_OFF_TEE", "SG_APPROACH", "SG_PUTTING","SG_AROUND_GREEN"),
funs("tenth" = . * 10))
sg_data2
fitmod <- function(data) lm(money_log ~
SG_OFF_TEE_tenth +
SG_APPROACH_tenth +
SG_PUTTING_tenth +
SG_AROUND_GREEN_tenth,
data = data)
sg_model <- sg_data2 %>%
group_by(year) %>%
nest() %>%
mutate(model = map(data, fitmod))
sg_model2 <- sg_model %>%
group_by(year)%>%
mutate(model_coef = map(model, broom::tidy),
model_glance = map(model, broom::glance))
sg_model2 %>%
unnest(model_coef) %>%
filter(term != "(Intercept)") %>%
ggplot(aes(year, exp(estimate) - 1, colour = term))+geom_line()+
scale_y_continuous(labels = percent,
"% Increase in Earnings from a 0.1 Strokes Gained Improvement")
sg_model2 %>%
unnest(model_coef) %>%
filter(term != "(Intercept)") %>%
ggplot(aes(year, exp(estimate) - 1, colour = term))+
geom_line()+
geom_point()+
scale_y_continuous(labels = percent,
"% Increase in Earnings from a 0.1 Strokes Gained Improvement")
sg_data
sg_data2
sg_data2 %>%
arrange(year) %>%
group_by(`PLAYER NAME`) %>%
mutate(SG_OFF_TEE_pct_change = (SG_OFF_TEE - lag(SG_OFF_TEE)) / lag(SG_OFF_TEE))
sg_data2 %>%
arrange(year) %>%
group_by(`PLAYER NAME`) %>%
mutate(SG_OFF_TEE_pct_change = (SG_OFF_TEE - lag(SG_OFF_TEE)) / lag(SG_OFF_TEE),
year_change = year - lag(year))
sg_data2 %>%
arrange(year) %>%
group_by(`PLAYER NAME`) %>%
mutate(SG_OFF_TEE_pct_change = (SG_OFF_TEE - lag(SG_OFF_TEE)) / lag(SG_OFF_TEE),
year_change = year - lag(year)) %>% View
sg_data2 %>%
arrange(year) %>%
group_by(`PLAYER NAME`) %>%
mutate(SG_OFF_TEE_pct_change = (SG_OFF_TEE - lag(SG_OFF_TEE)) / lag(SG_OFF_TEE),
year_change = year - lag(year),
SG_OFF_TEE_lag = lag(SG_OFF_TEE)) %>%
ungroup() %>%
filter(year_change == 1)
sg_data2 %>%
arrange(year) %>%
group_by(`PLAYER NAME`) %>%
mutate(SG_OFF_TEE_pct_change = (SG_OFF_TEE - lag(SG_OFF_TEE)) / lag(SG_OFF_TEE),
year_change = year - lag(year),
SG_OFF_TEE_lag = lag(SG_OFF_TEE)) %>%
ungroup() %>%
filter(year_change == 1) %>%
ggplot(aes(SG_OFF_TEE_lag, SG_OFF_TEE)) + geom_point()
sg_data2 %>%
arrange(year) %>%
group_by(`PLAYER NAME`) %>%
mutate(SG_OFF_TEE_pct_change = (SG_OFF_TEE - lag(SG_OFF_TEE)) / lag(SG_OFF_TEE),
year_change = year - lag(year),
SG_OFF_TEE_lag = lag(SG_OFF_TEE)) %>%
ungroup() %>%
filter(year_change == 1) %>%
ggplot(aes(SG_OFF_TEE_lag, SG_OFF_TEE)) + geom_point(aes(colour = SG_OFF_TEE_pct_change))
sg_data2 %>%
arrange(year) %>%
group_by(`PLAYER NAME`) %>%
mutate(SG_OFF_TEE_pct_change = (SG_OFF_TEE - lag(SG_OFF_TEE)) / lag(SG_OFF_TEE),
year_change = year - lag(year),
SG_OFF_TEE_lag = lag(SG_OFF_TEE)) %>%
ungroup() %>%
filter(year_change == 1) %>%
View()
getwd()
install.packages('keras')
library(keras)
model <- keras_model_sequential()
model %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dropout(rate = .5) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dropout(rate = .3) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dropout(rate = .1) %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = c('accuracy')
)
model %>%
fit(train_X_mat,
as.matrix(train_y_top50_perc_binary),
validation_data = list(test_X_mat,
as.matrix(test_y_top50_perc_binary)),
epochs = 25,
batch_size = 8)
library(tidyverse)
library(glmnet)
library(e1071)
library(randomForest)
library(keras)
check_accuracy <- function(actual_labels, pred_labels){
tt <- table(actual_labels, pred_labels)
sum(diag(tt)) / sum(tt)
}
full_model_data <- read_csv("data//modeling_data.csv")
set.seed(1)
training_samps <- sample(nrow(full_model_data),
0.75 * nrow(full_model_data))
# create training data
train_data <- full_model_data[training_samps,]
train_X <- train_data %>%
select(contains("lag"))
train_X_lag1 <- train_data %>%
select(contains("lag_1"))
train_X_lag1_2 <- train_data %>%
select(contains("lag_1"), contains("lag_2"), contains("lag12_delta"))
train_X_mat <- as.matrix(train_X)
train_X_mat_lag1 <- as.matrix(train_X_lag1)
train_X_mat_lag1_2 <- as.matrix(train_X_lag1_2)
train_y <- train_data$quartile_group
train_y_top50_perc_binary <- as.numeric(train_data$rank_money >= 0.5)
train_y_top50_perc_flag <- as.character(train_data$rank_money >= 0.5)
train_y_top50_perc_flag_factor <- as.factor(train_y_top50_perc_flag)
# create testing data
test_data <- full_model_data[-training_samps,]
test_X <- test_data %>%
select(contains("lag"))
test_X_lag1 <- test_data %>%
select(contains("lag_1"))
test_X_lag1_2 <- test_data %>%
select(contains("lag_1"), contains("lag_2"), contains("lag12_delta"))
test_X_mat <- as.matrix(test_X)
test_X_mat_lag1 <- as.matrix(test_X_lag1)
test_X_mat_lag1_2 <- as.matrix(test_X_lag1_2)
test_Y <- test_data$quartile_group
test_y_top50_perc_binary <- as.numeric(test_data$rank_money >= 0.5)
test_y_top50_perc_flag <- as.character(test_data$rank_money >= 0.5)
test_y_top50_perc_flag_factor <- as.factor(test_y_top50_perc_flag)
# train glmnet models
cv_glmn_full <- cv.glmnet(x = train_X_mat,
y = train_y_top50_perc_flag,
family = 'binomial',
nlambda = 500)
cv_glmn_lag1 <- cv.glmnet(x = train_X_mat_lag1,
y = train_y_top50_perc_flag,
family = 'binomial',
nlambda = 500)
cv_glmn_lag1_2 <- cv.glmnet(x = train_X_mat_lag1_2,
y = train_y_top50_perc_flag,
family = 'binomial',
nlambda = 500)
# train randomForest models
rf_full <- randomForest(x = train_X_mat,
y = train_y_top50_perc_flag_factor,
ntree = 1000,
importance = TRUE)
rf_lag1 <- randomForest(x = train_X_mat_lag1,
y = train_y_top50_perc_flag_factor,
ntree = 1000,
importance = TRUE)
rf_lag1_2 <- randomForest(x = train_X_mat_lag1_2,
y = train_y_top50_perc_flag_factor,
ntree = 1000,
importance = TRUE)
setwd("~/files_for_blog/scrape_pga_data")
library(tidyverse)
library(glmnet)
library(e1071)
library(randomForest)
library(keras)
check_accuracy <- function(actual_labels, pred_labels){
tt <- table(actual_labels, pred_labels)
sum(diag(tt)) / sum(tt)
}
full_model_data <- read_csv("data//modeling_data.csv")
set.seed(1)
training_samps <- sample(nrow(full_model_data),
0.75 * nrow(full_model_data))
# create training data
train_data <- full_model_data[training_samps,]
train_X <- train_data %>%
select(contains("lag"))
train_X_lag1 <- train_data %>%
select(contains("lag_1"))
train_X_lag1_2 <- train_data %>%
select(contains("lag_1"), contains("lag_2"), contains("lag12_delta"))
train_X_mat <- as.matrix(train_X)
train_X_mat_lag1 <- as.matrix(train_X_lag1)
train_X_mat_lag1_2 <- as.matrix(train_X_lag1_2)
train_y <- train_data$quartile_group
train_y_top50_perc_binary <- as.numeric(train_data$rank_money >= 0.5)
train_y_top50_perc_flag <- as.character(train_data$rank_money >= 0.5)
train_y_top50_perc_flag_factor <- as.factor(train_y_top50_perc_flag)
# create testing data
test_data <- full_model_data[-training_samps,]
test_X <- test_data %>%
select(contains("lag"))
test_X_lag1 <- test_data %>%
select(contains("lag_1"))
test_X_lag1_2 <- test_data %>%
select(contains("lag_1"), contains("lag_2"), contains("lag12_delta"))
test_X_mat <- as.matrix(test_X)
test_X_mat_lag1 <- as.matrix(test_X_lag1)
test_X_mat_lag1_2 <- as.matrix(test_X_lag1_2)
test_Y <- test_data$quartile_group
test_y_top50_perc_binary <- as.numeric(test_data$rank_money >= 0.5)
test_y_top50_perc_flag <- as.character(test_data$rank_money >= 0.5)
test_y_top50_perc_flag_factor <- as.factor(test_y_top50_perc_flag)
# train glmnet models
cv_glmn_full <- cv.glmnet(x = train_X_mat,
y = train_y_top50_perc_flag,
family = 'binomial',
nlambda = 500)
cv_glmn_lag1 <- cv.glmnet(x = train_X_mat_lag1,
y = train_y_top50_perc_flag,
family = 'binomial',
nlambda = 500)
cv_glmn_lag1_2 <- cv.glmnet(x = train_X_mat_lag1_2,
y = train_y_top50_perc_flag,
family = 'binomial',
nlambda = 500)
# train randomForest models
rf_full <- randomForest(x = train_X_mat,
y = train_y_top50_perc_flag_factor,
ntree = 1000,
importance = TRUE)
rf_lag1 <- randomForest(x = train_X_mat_lag1,
y = train_y_top50_perc_flag_factor,
ntree = 1000,
importance = TRUE)
rf_lag1_2 <- randomForest(x = train_X_mat_lag1_2,
y = train_y_top50_perc_flag_factor,
ntree = 1000,
importance = TRUE)
# train neural net
min_max_matrix <- function(mat){
apply(mat, 2, FUN = function(x) (x - min(x)) / (max(x) - min(x)))
}
model <- keras_model_sequential()
model %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dropout(rate = .5) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dropout(rate = .3) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dropout(rate = .1) %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = c('accuracy')
)
model %>%
fit(train_X_mat,
as.matrix(train_y_top50_perc_binary),
validation_data = list(test_X_mat,
as.matrix(test_y_top50_perc_binary)),
epochs = 25,
batch_size = 8)
cv_glmn_full$lambda.1se
cv_glmn_full$lambda.min
cv_glmn_lag1$lambda.1se
cv_glmn_lag1$lambda.min
?cv.glmnet
cv_glmn_lag1_2$lambda.min
cv_glmn_lag1_2$lambda.1se
lambda_glmn <- 0.035
glm_full <- glmnet(x = train_X_mat,
y = train_y_top50_perc_flag,
family = 'binomial',
lambda = lambda_glmn)
glm_full <- glmnet(x = train_X_mat,
y = train_y_top50_perc_flag,
family = 'binomial',
lambda = lambda_glmn)
glm_lag1 <- glmnet(x = train_X_mat_lag1,
y = train_y_top50_perc_flag,
family = 'binomial',
lambda = lambda_glmn)
glm_lag2 <- glmnet(x = train_X_mat_lag1_2,
y = train_y_top50_perc_flag,
family = 'binomial',
lambda = lambda_glmn)
predict(glm_full, newx = test_X_mat)
predict(glm_full, newx = test_X_mat, type = 'class')
predict(glm_full, newx = test_X_mat, type = 'prob')
predict(glm_full, newx = test_X_mat, type = 'response')
check_accuracy <- function(actual_labels, pred_labels){
tt <- table(actual_labels, pred_labels)
sum(diag(tt)) / sum(tt)
}
check_accuracy(test_y_top50_perc_flag,
predict(glm_full, newx = test_X_mat, type = 'class'))
check_accuracy(test_y_top50_perc_flag,
predict(glm_lag1, newx = test_X_mat, type = 'class'))
check_accuracy(test_y_top50_perc_flag,
predict(glm_lag1, newx = test_X_mat_lag1, type = 'class'))
check_accuracy(test_y_top50_perc_flag,
predict(glm_lag2, newx = test_X_mat_lag1_2, type = 'class'))
predict(rf_full, newdata = test_X_mat)
check_accuracy(test_y_top50_perc_flag_factor,
predict(rf_full, newdata = test_X_mat))
check_accuracy(test_y_top50_perc_flag_factor,
predict(rf_lag1, newdata = test_X_mat))
check_accuracy(test_y_top50_perc_flag_factor,
predict(rf_lag1_2, newdata = test_X_mat_lag1_2))
predict(model, newdata = test_X_mat)
?predict.keras.engine.training.Model
predict(model, x = test_X_mat)
predict(model, x = test_X_mat) >= 0.5
check_accuracy(test_y_top50_perc_binary, predict(model, x = test_X_mat) >= 0.5)
model <- keras_model_sequential()
model %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dropout(rate = .5) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dropout(rate = .3) %>%
# layer_dense(units = 16, activation = 'relu') %>%
# layer_dropout(rate = .1) %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = c('accuracy')
)
model %>%
fit(train_X_mat,
as.matrix(train_y_top50_perc_binary),
validation_data = list(test_X_mat,
as.matrix(test_y_top50_perc_binary)),
epochs = 25,
batch_size = 8)
dim(train_X)
model %>%
layer_dense(input_shape = 30, units = 64, activation = 'relu') %>%
layer_dropout(rate = .5) %>%
layer_dense(units = 16, activation = 'relu') %4>%
layer_dropout(rate = .3) %>%
# layer_dense(units = 16, activation = 'relu') %>%
# layer_dropout(rate = .1) %>%
layer_dense(units = 1, activation = "sigmoid")
model <- keras_model_sequential()
model %>%
layer_dense(input_shape = 30, units = 64, activation = 'relu') %>%
layer_dropout(rate = .5) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dropout(rate = .3) %>%
# layer_dense(units = 16, activation = 'relu') %>%
# layer_dropout(rate = .1) %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = c('accuracy')
)
model %>%
fit(train_X_mat,
as.matrix(train_y_top50_perc_binary),
validation_data = list(test_X_mat,
as.matrix(test_y_top50_perc_binary)),
epochs = 25,
batch_size = 8)
model %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dropout(rate = .5) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dropout(rate = .5) %>%
# layer_dense(units = 16, activation = 'relu') %>%
# layer_dropout(rate = .1) %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = c('accuracy')
)
model %>%
fit(train_X_mat,
as.matrix(train_y_top50_perc_binary),
validation_data = list(test_X_mat,
as.matrix(test_y_top50_perc_binary)),
epochs = 25,
batch_size = 8)
model <- keras_model_sequential()
model %>%
layer_dense(units = 32, activation = 'relu') %>%
layer_dropout(rate = .5) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dropout(rate = .3) %>%
# layer_dense(units = 16, activation = 'relu') %>%
# layer_dropout(rate = .1) %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = c('accuracy')
)
model %>%
fit(train_X_mat,
as.matrix(train_y_top50_perc_binary),
validation_data = list(test_X_mat,
as.matrix(test_y_top50_perc_binary)),
epochs = 25,
batch_size = 8)
